{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "754c99df",
   "metadata": {},
   "source": [
    "# <p align=\"center\">Siemens Sales Forecast</p>\n",
    "\n",
    "---\n",
    "\n",
    "## <p align=\"center\">*2 - Feature Selection & Modeling*</p>\n",
    "\n",
    "---\n",
    "\n",
    "### üë• **Team Members**\n",
    "- **Ana Farinha** *(Student Number: 20211514)*  \n",
    "- **Ant√≥nio Oliveira** *(Student Number: 20211595)*  \n",
    "- **Mariana Neto** *(Student Number: 20211527)*  \n",
    "- **Salvador Domingues** *(Student Number: 20240597)*  \n",
    "\n",
    "üìÖ **Date:** *April 1, 2025*  \n",
    "üìç **Prepared for:** *Siemens*  \n",
    "\n",
    "**GitHub Repo:** https://github.com/MGN19/Siemens-forecast\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b217ae9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T14:48:52.933704Z",
     "start_time": "2025-03-14T14:48:52.929415Z"
    }
   },
   "source": [
    "# ToC\n",
    "\n",
    "<a class=\"anchor\" id=\"top\"></a>\n",
    "\n",
    "\n",
    "1. [Import Libraries & Data](#1.-Import-Libraries-&-Data) <br><br>\n",
    "\n",
    "2. [Product Category #1](#Product-Category-#1) <br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6957c6f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T15:11:53.524977Z",
     "start_time": "2025-03-23T15:11:53.522506Z"
    }
   },
   "outputs": [],
   "source": [
    "## CELL TYPES (remover depois)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ea5f62",
   "metadata": {},
   "source": [
    "<div class=\"alert-danger\">\n",
    "    \n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b3ea15",
   "metadata": {},
   "source": [
    "<div class=\"alert-warning\">\n",
    "    \n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b453a2e",
   "metadata": {},
   "source": [
    "<div class=\"alert-info\">\n",
    "    \n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9202c2",
   "metadata": {},
   "source": [
    "# 1. Import Libraries & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd9d458a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T09:14:20.043887Z",
     "start_time": "2025-03-26T09:14:10.721391Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Suppress Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import fs_modelling as m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afd9fc0",
   "metadata": {},
   "source": [
    "**Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f34b24fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T09:14:22.120125Z",
     "start_time": "2025-03-26T09:14:22.030648Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded y_train_36\n",
      "Loaded y_train_8\n",
      "Loaded y_train_20\n",
      "Loaded y_train_9\n",
      "Loaded y_train_4\n",
      "Loaded y_train_11\n",
      "Loaded y_train_5\n",
      "Loaded y_train_12\n",
      "Loaded y_train_13\n",
      "Loaded y_train_6\n",
      "Loaded y_train_16\n",
      "Loaded y_train_3\n",
      "Loaded y_train_1\n",
      "Loaded y_train_14\n",
      "Loaded y_val_1\n",
      "Loaded y_val_3\n",
      "Loaded y_val_6\n",
      "Loaded y_val_5\n",
      "Loaded y_val_4\n",
      "Loaded y_val_16\n",
      "Loaded y_val_14\n",
      "Loaded y_val_11\n",
      "Loaded y_val_13\n",
      "Loaded y_val_12\n",
      "Loaded y_val_36\n",
      "Loaded y_val_20\n",
      "Loaded y_val_9\n",
      "Loaded y_val_8\n"
     ]
    }
   ],
   "source": [
    "X_train = pd.read_csv('./data/X_train_data/X_train.csv', index_col = 'Unnamed: 0')\n",
    "X_val = pd.read_csv('./data/X_val_data/X_val.csv', index_col = 'Unnamed: 0')\n",
    "\n",
    "def import_all_csvs_as_vars(folder):\n",
    "    for file in os.listdir(folder):\n",
    "        if file.endswith('.csv'):\n",
    "            df_name = file.replace('.csv', '')\n",
    "            df = pd.read_csv(os.path.join(folder, file))\n",
    "            globals()[df_name] = df\n",
    "            print(f\"Loaded {df_name}\")\n",
    "\n",
    "# Import each CSV file as individual DataFrames\n",
    "import_all_csvs_as_vars('data/y_train_data')\n",
    "import_all_csvs_as_vars('data/y_val_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c998ca9d",
   "metadata": {},
   "source": [
    "# Product Category #1\n",
    "\n",
    "<a href=\"#top\">Top &#129033;</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45896a12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T09:14:23.273517Z",
     "start_time": "2025-03-26T09:14:23.270998Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datasets = [X_train, X_val, y_train_1, y_val_1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238cb759",
   "metadata": {},
   "source": [
    "**Scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35cad259",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T09:14:30.675511Z",
     "start_time": "2025-03-26T09:14:30.665757Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_scaled, X_val_scaled = m.scale_data(X_train, \n",
    "                                          X_val, \n",
    "                                          scaler_type='minmax')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e30c99",
   "metadata": {},
   "source": [
    "## 2.1 Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c463d33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T09:29:43.974670Z",
     "start_time": "2025-03-26T09:29:43.966864Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectFdr, f_regression, RFECV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression, LassoCV\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "def feature_selection(X_train, y_train, method='all',\n",
    "                      rfe_model=None, corr_threshold=0.85, \n",
    "                      importance_threshold='mean', lasso_cv=True):\n",
    "    selected_features = []\n",
    "\n",
    "    if method == 'correlation':\n",
    "        corr_matrix = X_train.corr()\n",
    "        upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "        correlated_features = [column for column in upper_triangle.columns if any(upper_triangle[column].abs() > corr_threshold)]\n",
    "        selected_features = [col for col in X_train.columns if col not in correlated_features]\n",
    "        print(f'Selected {len(selected_features)} features by correlation')\n",
    "        \n",
    "    elif method == 'rfe':\n",
    "        if rfe_model is None:\n",
    "            rfe_model = LinearRegression()\n",
    "            \n",
    "        rfecv = RFECV(estimator=rfe_model, cv=5, scoring='neg_root_mean_squared_error')\n",
    "        rfecv.fit(X_train, y_train)\n",
    "        selected_features = X_train.columns[rfecv.support_]\n",
    "        print(f'Selected {len(selected_features)} features by RFECV')\n",
    "\n",
    "    elif method == 'importance':\n",
    "        model = RandomForestRegressor()\n",
    "        model.fit(X_train, y_train)\n",
    "        importances = model.feature_importances_\n",
    "\n",
    "        if importance_threshold == 'mean':\n",
    "            threshold_value = np.mean(importances)\n",
    "        elif importance_threshold == 'median':\n",
    "            threshold_value = np.median(importances)\n",
    "        else:\n",
    "            threshold_value = float(importance_threshold)\n",
    "\n",
    "        selected_features = X_train.columns[importances > threshold_value]\n",
    "        print(f'Selected {len(selected_features)} features by importance with threshold {threshold_value}')\n",
    "\n",
    "    elif method == 'lasso':\n",
    "        lasso = LassoCV(cv=5, random_state=42).fit(X_train, y_train)\n",
    "        selected_features = X_train.columns[lasso.coef_ != 0].tolist()\n",
    "        print(f'Selected {len(selected_features)} features by Lasso regularization')\n",
    "\n",
    "    elif method == 'all':\n",
    "        corr_features = feature_selection(X_train, y_train, method='correlation', corr_threshold=corr_threshold)\n",
    "        rfe_features = feature_selection(X_train, y_train, method='rfe', rfe_model=rfe_model)\n",
    "        importance_features = feature_selection(X_train, y_train, method='importance', importance_threshold=importance_threshold)\n",
    "        lasso_features = feature_selection(X_train, y_train, method='lasso')\n",
    "\n",
    "        selected_features = list(set(corr_features) & set(rfe_features) & set(importance_features) & set(lasso_features))\n",
    "        print(f'Selected {len(selected_features)} features that intersect across all methods')\n",
    "\n",
    "    return selected_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0686a3b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T09:31:32.885240Z",
     "start_time": "2025-03-26T09:30:21.592799Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 41 features by correlation\n",
      "Selected 3 features by RFECV\n",
      "Selected 27 features by importance with threshold 0.008130081300813009\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nLassoCV does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m selected_features = \u001b[43mfeature_selection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mall\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43mrfe_model\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mXGBRegressor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43mcorr_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.85\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43mimportance_threshold\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmean\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m selected_features\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mfeature_selection\u001b[39m\u001b[34m(X_train, y_train, method, rfe_model, corr_threshold, importance_threshold, lasso_cv)\u001b[39m\n\u001b[32m     51\u001b[39m rfe_features = feature_selection(X_train, y_train, method=\u001b[33m'\u001b[39m\u001b[33mrfe\u001b[39m\u001b[33m'\u001b[39m, rfe_model=rfe_model)\n\u001b[32m     52\u001b[39m importance_features = feature_selection(X_train, y_train, method=\u001b[33m'\u001b[39m\u001b[33mimportance\u001b[39m\u001b[33m'\u001b[39m, importance_threshold=importance_threshold)\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m lasso_features = \u001b[43mfeature_selection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlasso\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m selected_features = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(corr_features) & \u001b[38;5;28mset\u001b[39m(rfe_features) & \u001b[38;5;28mset\u001b[39m(importance_features) & \u001b[38;5;28mset\u001b[39m(lasso_features))\n\u001b[32m     56\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mSelected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(selected_features)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features that intersect across all methods\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mfeature_selection\u001b[39m\u001b[34m(X_train, y_train, method, rfe_model, corr_threshold, importance_threshold, lasso_cv)\u001b[39m\n\u001b[32m     42\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mSelected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(selected_features)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features by importance with threshold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthreshold_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m method == \u001b[33m'\u001b[39m\u001b[33mlasso\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     lasso = \u001b[43mLassoCV\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m     selected_features = X_train.columns[lasso.coef_ != \u001b[32m0\u001b[39m].tolist()\n\u001b[32m     47\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mSelected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(selected_features)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features by Lasso regularization\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/bcwds2/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:2129\u001b[39m, in \u001b[36mLassoCV.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, **params)\u001b[39m\n\u001b[32m   2091\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight=\u001b[38;5;28;01mNone\u001b[39;00m, **params):\n\u001b[32m   2092\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Fit Lasso model with coordinate descent.\u001b[39;00m\n\u001b[32m   2093\u001b[39m \n\u001b[32m   2094\u001b[39m \u001b[33;03m    Fit is on grid of alphas and best alpha estimated by cross-validation.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2127\u001b[39m \u001b[33;03m        Returns an instance of fitted model.\u001b[39;00m\n\u001b[32m   2128\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2129\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/bcwds2/lib/python3.13/site-packages/sklearn/base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/bcwds2/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:1652\u001b[39m, in \u001b[36mLinearModelCV.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, **params)\u001b[39m\n\u001b[32m   1640\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1641\u001b[39m     \u001b[38;5;66;03m# Need to validate separately here.\u001b[39;00m\n\u001b[32m   1642\u001b[39m     \u001b[38;5;66;03m# We can't pass multi_output=True because that would allow y to be\u001b[39;00m\n\u001b[32m   1643\u001b[39m     \u001b[38;5;66;03m# csr. We also want to allow y to be 64 or 32 but check_X_y only\u001b[39;00m\n\u001b[32m   1644\u001b[39m     \u001b[38;5;66;03m# allows to convert for 64.\u001b[39;00m\n\u001b[32m   1645\u001b[39m     check_X_params = \u001b[38;5;28mdict\u001b[39m(\n\u001b[32m   1646\u001b[39m         accept_sparse=\u001b[33m\"\u001b[39m\u001b[33mcsc\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1647\u001b[39m         dtype=[np.float64, np.float32],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1650\u001b[39m         copy=copy_X,\n\u001b[32m   1651\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1652\u001b[39m     X, y = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1653\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_separately\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheck_X_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_y_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1654\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1655\u001b[39m     copy_X = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1657\u001b[39m check_consistent_length(X, y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/bcwds2/lib/python3.13/site-packages/sklearn/utils/validation.py:2956\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2954\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mestimator\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m check_X_params:\n\u001b[32m   2955\u001b[39m     check_X_params = {**default_check_params, **check_X_params}\n\u001b[32m-> \u001b[39m\u001b[32m2956\u001b[39m X = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_X_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2957\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mestimator\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m check_y_params:\n\u001b[32m   2958\u001b[39m     check_y_params = {**default_check_params, **check_y_params}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/bcwds2/lib/python3.13/site-packages/sklearn/utils/validation.py:1107\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1102\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m expected <= 2.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1103\u001b[39m         % (array.ndim, estimator_name)\n\u001b[32m   1104\u001b[39m     )\n\u001b[32m   1106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[32m-> \u001b[39m\u001b[32m1107\u001b[39m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1111\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[32m   1115\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[32m   1116\u001b[39m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/bcwds2/lib/python3.13/site-packages/sklearn/utils/validation.py:120\u001b[39m, in \u001b[36m_assert_all_finite\u001b[39m\u001b[34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/bcwds2/lib/python3.13/site-packages/sklearn/utils/validation.py:169\u001b[39m, in \u001b[36m_assert_all_finite_element_wise\u001b[39m\u001b[34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name == \u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[32m    155\u001b[39m     msg_err += (\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not accept missing values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m#estimators-that-handle-nan-values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    168\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[31mValueError\u001b[39m: Input X contains NaN.\nLassoCV does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "selected_features = feature_selection(X_train_scaled, y_train_1, \n",
    "                                      method='all', \n",
    "                                      rfe_model = XGBRegressor(), \n",
    "                                      corr_threshold=0.85, \n",
    "                                      importance_threshold = 'mean')\n",
    "\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3b75280f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T09:24:13.030112Z",
     "start_time": "2025-03-26T09:24:10.638036Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from lazypredict.Supervised import LazyRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from prophet import Prophet  # Prophet model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statsmodels.tsa.arima.model import ARIMA  # ARIMA model\n",
    "\n",
    "def modelling(X_train, y_train, X_test, y_test, \n",
    "              features_used, \n",
    "              metric='RMSE', \n",
    "              model_choice='arima', \n",
    "              save_path='./modelling_csvs/results.csv'):\n",
    "\n",
    "\n",
    "    # If ARIMA is chosen\n",
    "    if model_choice == 'arima':\n",
    "        if len(features_used) > 1:\n",
    "            raise ValueError('ARIMA only accepts 1 feature for the target variable.')\n",
    "        model = ARIMA(y_train, order=(1, 1, 1))\n",
    "        model_fit = model.fit()\n",
    "\n",
    "        # Predict\n",
    "        predictions = model_fit.forecast(len(y_test))\n",
    "        \n",
    "        # Calculate RMSE or any other metric\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "        best_model_name = 'ARIMA'\n",
    "        best_score = rmse\n",
    "        print(f'ARIMA RMSE: {rmse}')\n",
    "\n",
    "    # Initialize LazyRegressor if 'lazy' model is chosen\n",
    "    elif model_choice == 'lazy':\n",
    "        regressor = LazyRegressor(verbose=0)\n",
    "        models, predictions = regressor.fit(X_train[features_used], X_test[features_used], y_train, y_test)\n",
    "        \n",
    "        # Select the best model based on the metric\n",
    "        best_model = models.sort_values(by=metric).iloc[0]\n",
    "        best_model_name = best_model.name\n",
    "        best_score = best_model[metric]\n",
    "        print(f'Best model: {best_model_name}')\n",
    "        print(f'{metric} of the best model: {best_score}')\n",
    "    \n",
    "    # If Prophet is chosen\n",
    "    elif model_choice == 'prophet':\n",
    "        raise ValueError('NOT WORKING YET')\n",
    "        # Initialize an empty DataFrame to hold future feature values\n",
    "        future_features = pd.DataFrame()\n",
    "\n",
    "        # Iterate through each column in 'features_used'\n",
    "        for column in features_used:\n",
    "            # Isolate the current column into a new DataFrame 'df1'\n",
    "            df1 = X_train[[column]].copy()\n",
    "\n",
    "            # Reset the index of 'df1' and rename columns to fit Prophet's expected format\n",
    "            data = (df1.reset_index()\n",
    "                    .rename(columns={'index': 'ds', f'{column}': 'y'}))\n",
    "\n",
    "            # Initialize Prophet model\n",
    "            model = Prophet()\n",
    "\n",
    "            # Fit the model to the data\n",
    "            model.fit(data)\n",
    "\n",
    "            # Create a DataFrame representing future dates to make predictions\n",
    "            future = model.make_future_dataframe(periods=10, freq='MS')\n",
    "\n",
    "            # Forecast future dates\n",
    "            forecast_index = model.predict(future)\n",
    "\n",
    "            forecast_index = forecast_index[['ds', 'yhat']]\n",
    "            \n",
    "            # Set the date column as the index\n",
    "            forecast_index = forecast_index.set_index('ds')\n",
    "\n",
    "            # Add the forecasted values to the 'future_features' DataFrame\n",
    "            future_features[column] = forecast_index['yhat'].values\n",
    "\n",
    "        # Reset the index to use date as a regular column\n",
    "        future_features.reset_index(inplace=True)\n",
    "\n",
    "        # Add the date column to 'future_features'\n",
    "        future_features['ds'] = forecast_index['ds'].values\n",
    "\n",
    "        # Set date as the index of 'future_features'\n",
    "        future_features.set_index('ds', inplace=True)\n",
    "\n",
    "        # For demonstration, using RMSE here:\n",
    "        predicted_values = future_features[features_used].mean(axis=1)  # For simplicity, take the mean of all predictions\n",
    "        rmse = mean_squared_error(y_test, predicted_values, squared=False)\n",
    "        \n",
    "        # Compare RMSE to get best model\n",
    "        if rmse < best_score:\n",
    "            best_score = rmse\n",
    "            best_model_name = 'prophet'\n",
    "            \n",
    "    # Prepare row to save\n",
    "    result_row = {\n",
    "        'Features Used': ', '.join(features_used),\n",
    "        'Best Model': best_model_name,\n",
    "        metric: best_score\n",
    "    }\n",
    "    \n",
    "    # Check if file exists; if not, create with headers\n",
    "    if not os.path.isfile(save_path):\n",
    "        results_df = pd.DataFrame([result_row])\n",
    "        results_df.to_csv(save_path, index=False)\n",
    "    else:\n",
    "        # Append without overwriting\n",
    "        results_df = pd.DataFrame([result_row])\n",
    "        results_df.to_csv(save_path, mode='a', header=False, index=False)\n",
    "    \n",
    "    return best_model_name, best_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "341ece87-c8ad-4b56-ba7f-ad5cdbd6260b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "NOT WORKING YET",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[103]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m best_model_name, best_score  = \u001b[43mmodelling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_1_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val_1_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m              \u001b[49m\u001b[43mfeatures_used\u001b[49m\u001b[43m=\u001b[49m\u001b[43mselected_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m              \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mRMSE\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m              \u001b[49m\u001b[43mmodel_choice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprophet\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[102]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36mmodelling\u001b[39m\u001b[34m(X_train, y_train, X_test, y_test, features_used, metric, model_choice, save_path)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# If Prophet is chosen\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m model_choice == \u001b[33m'\u001b[39m\u001b[33mprophet\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mNOT WORKING YET\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     50\u001b[39m     \u001b[38;5;66;03m# Initialize an empty DataFrame to hold future feature values\u001b[39;00m\n\u001b[32m     51\u001b[39m     future_features = pd.DataFrame()\n",
      "\u001b[31mValueError\u001b[39m: NOT WORKING YET"
     ]
    }
   ],
   "source": [
    "best_model_name, best_score  = modelling(X_train_1_scaled, y_train_1, X_val_1_scaled, y_val_1, \n",
    "              features_used=selected_features, \n",
    "              metric='RMSE', \n",
    "              model_choice='prophet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fb18eb30-2c80-4ea0-8374-87151d56e0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prophet import Prophet\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def prophet_forecast(X_train, y_train, features_used, periods=10, freq='MS'):\n",
    "    \"\"\"\n",
    "    Function to perform forecasting using Prophet for each feature in 'features_used'.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train: DataFrame containing the training data\n",
    "    - y_train: Actual ground truth values for RMSE calculation (training data)\n",
    "    - features_used: List of features/columns to forecast using Prophet\n",
    "    - periods: Number of future periods to predict (default=10)\n",
    "    - freq: Frequency of the periods (default='MS' for monthly start)\n",
    "\n",
    "    Returns:\n",
    "    - best_model_name: Name of the best model ('prophet')\n",
    "    - best_score: The lowest RMSE score for the model\n",
    "    \"\"\"\n",
    "    best_score = float('inf')\n",
    "    best_model_name = None\n",
    "\n",
    "    # Initialize an empty DataFrame to hold future feature values\n",
    "    future_features = pd.DataFrame()\n",
    "\n",
    "    # Iterate through each column in 'features_used'\n",
    "    for column in features_used:\n",
    "        # Isolate the current column into a new DataFrame 'df1'\n",
    "        df1 = X_train[[column]].copy()\n",
    "\n",
    "        # Reset the index of 'df1' and rename columns to fit Prophet's expected format\n",
    "        data = (df1.reset_index()\n",
    "                .rename(columns={'index': 'ds', f'{column}': 'y'}))\n",
    "\n",
    "        # Initialize Prophet model\n",
    "        model = Prophet()\n",
    "\n",
    "        # Fit the model to the data\n",
    "        model.fit(data)\n",
    "\n",
    "        # Create a DataFrame representing future dates to make predictions\n",
    "        future = model.make_future_dataframe(periods=periods, freq=freq)\n",
    "\n",
    "        # Forecast future dates\n",
    "        forecast_index = model.predict(future)\n",
    "\n",
    "        # Select relevant columns ('ds' for date, 'yhat' for predictions)\n",
    "        forecast_index = forecast_index[['ds', 'yhat']]\n",
    "\n",
    "        # Set the date column as the index\n",
    "        forecast_index = forecast_index.set_index('ds')\n",
    "\n",
    "        # Add the forecasted values to the 'future_features' DataFrame\n",
    "        future_features[column] = forecast_index['yhat'].values\n",
    "\n",
    "    # Reset the index of the future_features DataFrame to use 'ds' as a regular column\n",
    "    future_features.reset_index(inplace=True)\n",
    "\n",
    "    # Add the date column to 'future_features'\n",
    "    future_features['ds'] = forecast_index.index.values\n",
    "\n",
    "    # Set 'ds' as the index of 'future_features'\n",
    "    future_features.set_index('ds', inplace=True)\n",
    "\n",
    "    # Ensure we only compare the forecasted values against a valid subset of y_train\n",
    "    # For simplicity, we will compare the mean of the forecasted values to the corresponding `y_train` values\n",
    "    predicted_values = future_features[features_used].mean(axis=1)  # For simplicity, take the mean of all predictions\n",
    "\n",
    "    rmse = mean_squared_error(y_train, predicted_values)\n",
    "\n",
    "    # Compare RMSE to get the best model\n",
    "    if rmse < best_score:\n",
    "        best_score = rmse\n",
    "        best_model_name = 'prophet'\n",
    "\n",
    "    return best_model_name, best_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8b5d595e-afad-4036-9532-7170a4592007",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:44:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:44:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:44:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:44:03 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [31, 41]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mprophet_forecast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_1_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_features\u001b[49m\u001b[43m)\u001b[49m \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 69\u001b[39m, in \u001b[36mprophet_forecast\u001b[39m\u001b[34m(X_train, y_train, features_used, periods, freq)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# Ensure we only compare the forecasted values against a valid subset of y_train\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# For simplicity, we will compare the mean of the forecasted values to the corresponding `y_train` values\u001b[39;00m\n\u001b[32m     67\u001b[39m predicted_values = future_features[features_used].mean(axis=\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# For simplicity, take the mean of all predictions\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m rmse = \u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredicted_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# Compare RMSE to get the best model\u001b[39;00m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m rmse < best_score:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/bcwds2/lib/python3.13/site-packages/sklearn/utils/_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/bcwds2/lib/python3.13/site-packages/sklearn/metrics/_regression.py:565\u001b[39m, in \u001b[36mmean_squared_error\u001b[39m\u001b[34m(y_true, y_pred, sample_weight, multioutput)\u001b[39m\n\u001b[32m    515\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Mean squared error regression loss.\u001b[39;00m\n\u001b[32m    516\u001b[39m \n\u001b[32m    517\u001b[39m \u001b[33;03mRead more in the :ref:`User Guide <mean_squared_error>`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    561\u001b[39m \u001b[33;03m0.825...\u001b[39;00m\n\u001b[32m    562\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    563\u001b[39m xp, _ = get_namespace(y_true, y_pred, sample_weight, multioutput)\n\u001b[32m    564\u001b[39m _, y_true, y_pred, sample_weight, multioutput = (\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m     \u001b[43m_check_reg_targets_with_floating_dtype\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    568\u001b[39m )\n\u001b[32m    569\u001b[39m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[32m    570\u001b[39m output_errors = _average((y_true - y_pred) ** \u001b[32m2\u001b[39m, axis=\u001b[32m0\u001b[39m, weights=sample_weight)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/bcwds2/lib/python3.13/site-packages/sklearn/metrics/_regression.py:198\u001b[39m, in \u001b[36m_check_reg_targets_with_floating_dtype\u001b[39m\u001b[34m(y_true, y_pred, sample_weight, multioutput, xp)\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Ensures that y_true, y_pred, and sample_weight correspond to the same\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[33;03mregression task.\u001b[39;00m\n\u001b[32m    150\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    194\u001b[39m \u001b[33;03m    correct keyword.\u001b[39;00m\n\u001b[32m    195\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    196\u001b[39m dtype_name = _find_matching_floating_dtype(y_true, y_pred, sample_weight, xp=xp)\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m y_type, y_true, y_pred, multioutput = \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[38;5;66;03m# _check_reg_targets does not accept sample_weight as input.\u001b[39;00m\n\u001b[32m    203\u001b[39m \u001b[38;5;66;03m# Convert sample_weight's data type separately to match dtype_name.\u001b[39;00m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/bcwds2/lib/python3.13/site-packages/sklearn/metrics/_regression.py:104\u001b[39m, in \u001b[36m_check_reg_targets\u001b[39m\u001b[34m(y_true, y_pred, multioutput, dtype, xp)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Check that y_true and y_pred belong to the same regression task.\u001b[39;00m\n\u001b[32m     60\u001b[39m \n\u001b[32m     61\u001b[39m \u001b[33;03mTo reduce redundancy when calling `_find_matching_floating_dtype`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    100\u001b[39m \u001b[33;03m    correct keyword.\u001b[39;00m\n\u001b[32m    101\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    102\u001b[39m xp, _ = get_namespace(y_true, y_pred, multioutput, xp=xp)\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m y_true = check_array(y_true, ensure_2d=\u001b[38;5;28;01mFalse\u001b[39;00m, dtype=dtype)\n\u001b[32m    106\u001b[39m y_pred = check_array(y_pred, ensure_2d=\u001b[38;5;28;01mFalse\u001b[39;00m, dtype=dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/bcwds2/lib/python3.13/site-packages/sklearn/utils/validation.py:475\u001b[39m, in \u001b[36mcheck_consistent_length\u001b[39m\u001b[34m(*arrays)\u001b[39m\n\u001b[32m    473\u001b[39m uniques = np.unique(lengths)\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    476\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    477\u001b[39m         % [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[32m    478\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Found input variables with inconsistent numbers of samples: [31, 41]"
     ]
    }
   ],
   "source": [
    "prophet_forecast(X_train_1_scaled, y_train_1, selected_features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd585af-d3c1-40a5-8034-45d5b0d76496",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3cd7e059-cb3c-436a-a042-c3061d431445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting statsmodels\n",
      "  Using cached statsmodels-0.14.4-cp313-cp313-macosx_10_13_x86_64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.22.3 in /opt/anaconda3/envs/bcwds2/lib/python3.13/site-packages (from statsmodels) (2.2.4)\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /opt/anaconda3/envs/bcwds2/lib/python3.13/site-packages (from statsmodels) (1.15.2)\n",
      "Requirement already satisfied: pandas!=2.1.0,>=1.4 in /opt/anaconda3/envs/bcwds2/lib/python3.13/site-packages (from statsmodels) (2.2.3)\n",
      "Collecting patsy>=0.5.6 (from statsmodels)\n",
      "  Using cached patsy-1.0.1-py2.py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: packaging>=21.3 in /opt/anaconda3/envs/bcwds2/lib/python3.13/site-packages (from statsmodels) (24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/bcwds2/lib/python3.13/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/bcwds2/lib/python3.13/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/bcwds2/lib/python3.13/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/bcwds2/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels) (1.17.0)\n",
      "Using cached statsmodels-0.14.4-cp313-cp313-macosx_10_13_x86_64.whl (10.2 MB)\n",
      "Using cached patsy-1.0.1-py2.py3-none-any.whl (232 kB)\n",
      "Installing collected packages: patsy, statsmodels\n",
      "Successfully installed patsy-1.0.1 statsmodels-0.14.4\n"
     ]
    }
   ],
   "source": [
    "!pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "855ac6bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T10:42:17.540624Z",
     "start_time": "2025-03-25T10:42:16.316552Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42/42 [00:01<00:00, 34.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 31, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 36289610.258065\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "Best model: SVR\n",
      "RMSE of the best model: 2887703.8751545465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_1_selected = X_train_1_scaled[selected_features]\n",
    "X_val_1_selected = X_val_1_scaled[selected_features]\n",
    "\n",
    "best_model_name, best_score = modelling(\n",
    "    X_train_1_selected, y_train_1, X_val_1_selected, y_val_1, \n",
    "    features_used=selected_features, metric='RMSE'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "18f4288a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T15:53:41.960098Z",
     "start_time": "2025-03-23T15:53:41.955735Z"
    }
   },
   "outputs": [],
   "source": [
    "# import lazypredict\n",
    "# from lazypredict.Supervised import LazyRegressor\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# import numpy as np\n",
    "\n",
    "# def lazy_regressor(X_train, y_train, X_test, y_test, metric='RMSE'):\n",
    "    \n",
    "#     # Initialize LazyRegressor\n",
    "#     regressor = LazyRegressor(verbose=0)\n",
    "    \n",
    "#     # Fit the model\n",
    "#     models, predictions = regressor.fit(X_train, X_test, y_train, y_test)\n",
    "    \n",
    "#     # Select the best model based on the metric (e.g., RMSE)\n",
    "#     best_model = models.sort_values(by=metric).iloc[0]\n",
    "    \n",
    "#     # Get the model name and the best score\n",
    "#     best_model_name = best_model.name\n",
    "#     best_score = best_model[metric]\n",
    "    \n",
    "#     print(f'Best model: {best_model_name}')\n",
    "#     print(f'{metric} of the best model: {best_score}')\n",
    "    \n",
    "#     # Return the best model and score\n",
    "#     return best_model_name, best_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a9110ab7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T15:53:44.734147Z",
     "start_time": "2025-03-23T15:53:42.340731Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42/42 [00:02<00:00, 17.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 31, number of used features: 0\n",
      "[LightGBM] [Info] Start training from score 36289610.258065\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "Best model: GradientBoostingRegressor\n",
      "RMSE of the best model: 2847141.438462039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# best_model_name, best_score = lazy_regressor(X_train_1_scaled, y_train_1,\n",
    "#                                              X_val_1_scaled, y_val_1,\n",
    "#                                              metric='RMSE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9cf56a51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T10:42:19.884930Z",
     "start_time": "2025-03-25T10:42:19.877181Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features Used</th>\n",
       "      <th>Best Model</th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(W) Price of Base Metals, GER Production Index...</td>\n",
       "      <td>SVR</td>\n",
       "      <td>2887703.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USA Shipments Index_Rolling_Mean_3</td>\n",
       "      <td>HuberRegressor</td>\n",
       "      <td>2823366.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BC_CHI, USA Shipments Index_Rolling_Mean_3, CH...</td>\n",
       "      <td>HuberRegressor</td>\n",
       "      <td>2830173.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(W) Price of Base Metals, GER Production Index...</td>\n",
       "      <td>SVR</td>\n",
       "      <td>2887703.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(W) Price of Natural gas index, USA Production...</td>\n",
       "      <td>SVR</td>\n",
       "      <td>2887703.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Features Used      Best Model  \\\n",
       "0  (W) Price of Base Metals, GER Production Index...             SVR   \n",
       "1                 USA Shipments Index_Rolling_Mean_3  HuberRegressor   \n",
       "2  BC_CHI, USA Shipments Index_Rolling_Mean_3, CH...  HuberRegressor   \n",
       "3  (W) Price of Base Metals, GER Production Index...             SVR   \n",
       "4  (W) Price of Natural gas index, USA Production...             SVR   \n",
       "\n",
       "        RMSE  \n",
       "0 2887703.96  \n",
       "1 2823366.38  \n",
       "2 2830173.62  \n",
       "3 2887703.92  \n",
       "4 2887703.88  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = pd.read_csv('./lazy_regressor_results.csv')\n",
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "85584d6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T16:02:28.834869Z",
     "start_time": "2025-03-23T16:02:28.801224Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'autosklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mautosklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maskl2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ForecasterAutoregMultiSeries\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'autosklearn'"
     ]
    }
   ],
   "source": [
    "from autosklearn.experimental.askl2 import ForecasterAutoregMultiSeries\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "def time_series_forecasting(X_train, y_train, X_test, y_test, \n",
    "                            forecast_steps=10, metric='neg_root_mean_squared_error'):\n",
    "\n",
    "    # Initialize ForecasterAutoregMultiSeries\n",
    "    forecaster = ForecasterAutoregMultiSeries(steps=forecast_steps, metric=metric)\n",
    "    \n",
    "    # Fit the forecaster with training data\n",
    "    forecaster.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = forecaster.predict(X_test)\n",
    "    \n",
    "    # Calculate RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    print(f'Test RMSE: {rmse}')\n",
    "    \n",
    "    return forecaster, y_pred, rmse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f633116",
   "metadata": {},
   "source": [
    "### 2.1.1 Filter Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1b2ec6",
   "metadata": {},
   "source": [
    "**Variance Threshold**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8bec66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T15:11:56.216711Z",
     "start_time": "2025-03-23T15:11:56.216700Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_1_scaled.var() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb5dced",
   "metadata": {},
   "source": [
    "**Spearman Correlation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2e397f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T15:11:56.218093Z",
     "start_time": "2025-03-23T15:11:56.218080Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def correlation_matrix(X, cmap='YlOrBr', font_size = 5):\n",
    "    \"\"\"\n",
    "    Input: X (numerical data)\n",
    "    Output: Correlation Matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    # Correlation matrix\n",
    "    corr_matrix = X.corr().abs()\n",
    "\n",
    "    # Plot Heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap=cmap, fmt=\".2f\", annot_kws={\"size\": font_size})\n",
    "    plt.title(\"Feature Correlation Matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379196d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T15:11:56.219727Z",
     "start_time": "2025-03-23T15:11:56.219713Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "correlation_matrix(X_train_1_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb5e958",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T15:11:56.220921Z",
     "start_time": "2025-03-23T15:11:56.220910Z"
    }
   },
   "outputs": [],
   "source": [
    "def top_correlated_features(X, top_n=20, cmap='YlOrBr'):\n",
    "    corr_matrix = X.corr().abs()\n",
    "    mean_corr = corr_matrix.mean().sort_values(ascending=False)\n",
    "    top_features = mean_corr.head(top_n).index\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(X[top_features].corr(), annot=True, cmap=cmap, fmt=\".2f\", annot_kws={\"size\": 8})\n",
    "    plt.title(f\"Top {top_n} Most Correlated Features\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c365f2a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T15:11:56.222515Z",
     "start_time": "2025-03-23T15:11:56.222501Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_correlated_features(X_train_1_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80988409",
   "metadata": {},
   "source": [
    "### 2.2.2 Wrapper Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed569a6",
   "metadata": {},
   "source": [
    "**RFE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f909060",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T15:11:56.224068Z",
     "start_time": "2025-03-23T15:11:56.224057Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "def rfe(X_train, y_train, X_val, y_val, n_features, model=None):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        X_train, y_train, X_val, y_val: training and validation data\n",
    "        n_features: number of features to use for RFE\n",
    "        model: chosen regression model \n",
    "    Output: selected features for the best model based on RMSE\n",
    "    \"\"\"\n",
    "    \n",
    "    best_rmse = np.inf  # Start with a very high RMSE (lower is better)\n",
    "    best_features = []\n",
    "\n",
    "    results = {}\n",
    "    \n",
    "    for feature in n_features:\n",
    "        \n",
    "        # Fit RFE\n",
    "        rfe = RFE(estimator=model, n_features_to_select=feature)\n",
    "        rfe.fit(X_train, y_train)\n",
    "\n",
    "        # Get selected features\n",
    "        selected_features = X_train.columns[rfe.support_]\n",
    "        \n",
    "        print('\\n-------------TRAIN-------------')\n",
    "\n",
    "        # Predictions for Train\n",
    "        y_pred_train = rfe.predict(X_train)\n",
    "        \n",
    "        # Metrics for Training (RMSE)\n",
    "        mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "        rmse_train = np.sqrt(mse_train)\n",
    "        \n",
    "        print(f\"RMSE for {feature} features (Train): {rmse_train:.4f}\")\n",
    "        \n",
    "        print('----------VALIDATION----------')\n",
    "\n",
    "        # Predictions for Validation\n",
    "        y_pred_val = rfe.predict(X_val)\n",
    "        \n",
    "        # Metrics for Validation (RMSE)\n",
    "        mse_val = mean_squared_error(y_val, y_pred_val)\n",
    "        rmse_val = np.sqrt(mse_val)\n",
    "        \n",
    "        print(f\"RMSE for {feature} features (Validation): {rmse_val:.4f}\")\n",
    "        \n",
    "        # Best score based on RMSE (lower is better)\n",
    "        if rmse_val < best_rmse:\n",
    "            best_rmse = rmse_val\n",
    "            best_features = selected_features.tolist()  \n",
    "        \n",
    "    print('\\n----------FINAL----------')\n",
    "    print(f'Best RMSE: {best_rmse}')\n",
    "    print(f'Number of Features: {len(best_features)}')\n",
    "    print(f'Features: {best_features}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c7a49f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T15:11:56.225027Z",
     "start_time": "2025-03-23T15:11:56.225016Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TEMP\n",
    "X_train_1_scaled.fillna(0, inplace = True)\n",
    "X_val_1_scaled.fillna(0, inplace = True)\n",
    "\n",
    "n_features = np.arange(1, len(X_train_1_scaled.columns) + 1)\n",
    "model = LinearRegression()\n",
    "\n",
    "rfe(X_train_1_scaled, y_train_1, X_val_1_scaled, y_val_1, \n",
    "                                n_features = n_features, \n",
    "                                model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79055c5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T15:11:56.226165Z",
     "start_time": "2025-03-23T15:11:56.226154Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TEMP\n",
    "X_train_1_scaled.fillna(0, inplace = True)\n",
    "X_val_1_scaled.fillna(0, inplace = True)\n",
    "\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "n_features = np.arange(1, len(X_train_1_scaled.columns) + 1)\n",
    "model = XGBRegressor(objective='reg:squarederror',\n",
    "                     random_state = 100)\n",
    "\n",
    "rfe(X_train_1_scaled, y_train_1, X_val_1_scaled, y_val_1, \n",
    "                                n_features = n_features, \n",
    "                                model = model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1954c798",
   "metadata": {},
   "source": [
    "### 2.2.3 Embedded Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3119f7",
   "metadata": {},
   "source": [
    "**LASSO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbeb421",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T15:11:56.227721Z",
     "start_time": "2025-03-23T15:11:56.227707Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "import functions as f\n",
    "def lasso(X, y, alpha = 0.01, color = f.main_color):\n",
    "\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        X, y: data\n",
    "        alpha: parameter for lasso\n",
    "    Output: Plot, initial and selected features\n",
    "    \"\"\"\n",
    "    \n",
    "    # Fit\n",
    "    lasso = Lasso(alpha=alpha)\n",
    "    lasso.fit(X, y)\n",
    "    \n",
    "    # Get Feature Importance\n",
    "    importance = pd.Series(lasso.coef_, index=X.columns)\n",
    "    importance.sort_values().plot(kind=\"barh\", color=color)\n",
    "    non_zero_importance = importance[importance != 0]\n",
    "    selected_features = non_zero_importance.index\n",
    "\n",
    "    # Plot\n",
    "    plt.title(\"Lasso Feature Importance\")\n",
    "    plt.xlabel(\"Coefficient Value\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Print Results\n",
    "    print(f\"\\nInitial Features: {len(X.columns)}\\n\")\n",
    "    print(X.columns.tolist())\n",
    "    print(f\"\\nDecision for Numerical Features (lasso ‚â† 0): {len(selected_features.tolist())}\\n\")\n",
    "    print(selected_features.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6614a16c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T15:11:56.228825Z",
     "start_time": "2025-03-23T15:11:56.228814Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lasso(X_train_1_scaled, y_train_1, alpha = 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2969aaa2",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a97773",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T15:11:56.230110Z",
     "start_time": "2025-03-23T15:11:56.230099Z"
    }
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the XGBoost model\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1)\n",
    "\n",
    "# Fit the model using the training data\n",
    "model.fit(X_train_1_scaled, y_train_1)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred_val_1 = model.predict(X_val_1_scaled)\n",
    "\n",
    "# Calculate RMSE for the validation set\n",
    "rmse_val_1 = np.sqrt(mean_squared_error(y_val_1, y_pred_val_1))\n",
    "print(f'Root Mean Squared Error on Validation Set: {rmse_val_1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295e9ca6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T15:11:56.231315Z",
     "start_time": "2025-03-23T15:11:56.231303Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(y_val_1.index, y_val_1, label='Actual Validation Values', linestyle='-', color='g')\n",
    "plt.plot(y_val_1.index, y_pred_val_1, label='Predicted Validation Values', linestyle='--', color='g')\n",
    "\n",
    "plt.plot(y_train_1.index, y_train_1, label='Actual Training Values', linestyle='-', color='b')\n",
    "\n",
    "plt.xlabel('Date/Time')\n",
    "plt.ylabel('Sales')\n",
    "plt.title('XGBoost Forecasting: Actual vs Predicted (Training and Validation)')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
